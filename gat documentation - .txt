gat documentation - 
day 1 - i started with reading the paper two times making notes and understanding what it meant.
day 2,3,4- I felt that i couldnt understand most of the concepts that were being used just by the paper (like i had terms but if i was told to code anything like that - i wouldn't have the clarity) so i read some blogs and the material given in the references, also watched videos about GNNs to  understand what different terms in the paper meant.
day 5 - I found this paper review by aleksa gordic (i think) online that really helped. and started with the tutorials on my vscode.
day 6 - the tutorial REALLY helped me with the matrices being used (i tend to get really confused if i have to work with a lot of them - such was true in this case). but i quickly realised that vscode was got viable for this and moved on to jupyter as i couldn't wait so long after every small error that i made.
day 7 - i switched to jupyter and completed my GAT layer (single headed) and GAT and loaded up PPI dataset as it was available with torch (big mistake)
day 8 - i completed the training and testing code but whenever i was running it, it kept crashing due to RAM. I tried again with just one layer and subset sampling but it kept crashing. then i found an actual tutorial of the code online, in the tutorial itself then person says that 12gb of RAM wont be enough, after this person had written a 2500 line code to optimize it. Thus, i gave up my persuit of using PPI and moved on to the molecular datasets.
day 9 - i downloaded some on the datasets quickly realising that these are just edge graphs with no nodal classes (and i didnot know how i will get a benchmark against these datasets) thus i asked harshwardhan and switched to CORA the next day.
day 10 - CORA is a relatively simple dataset thus i was able to mimic the implementation the paper suggested and got a 54 percent test accuracy, i used the exact way of getting transductive predictions that the paper used (20 nodes from each class for training, 2 layered GAT). Thus, i realised that i need to move towards multihead.
day 11,12 - I spent these two days just trying to get multiattention right even though its a simple thing to grasp and implement, I just kept getting ram crashes and tensor shapes not matching up (one problem that showed up a LOT was a tensor accidentally getting an extra dimension with i length like [[1,2,3]]. and in the end after running it with 8 heads and 1 head it gives me a 71 percent accuracy.
