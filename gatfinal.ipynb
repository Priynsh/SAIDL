{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655d51eb-aca9-4d59-a250-cfdbd698321d",
      "metadata": {
        "id": "655d51eb-aca9-4d59-a250-cfdbd698321d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import os\n",
        "from torch_geometric.datasets import PPI\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout, alpha,head, concat=True):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.dropout       = dropout\n",
        "        self.in_features   = in_features\n",
        "        self.out_features  = out_features\n",
        "        self.alpha         = alpha\n",
        "        self.concat        = concat\n",
        "        self.head = head\n",
        "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        self.a = nn.Parameter(torch.zeros(size=(self.head,2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, input, edge_index):\n",
        "        h = torch.mm(input, self.W)\n",
        "        N = h.size()[0]\n",
        "        adj1 = to_dense_adj(edge_index)\n",
        "        adj1=adj1[0]\n",
        "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e = torch.zeros(self.head,2708,2708)\n",
        "        for i in range(self.head):\n",
        "          e[i] = self.leakyrelu(torch.matmul(a_input, self.a[i]).squeeze(2))\n",
        "        # print(e.shape)\n",
        "        # print(\"u\")\n",
        "        zero_vec = -9e15 * torch.ones_like(e)\n",
        "        attention  = torch.zeros_like(e)\n",
        "        # print(attention.shape)\n",
        "        for i  in range(self.head):\n",
        "          attention[i] = torch.where(adj1 > 0, e[i], zero_vec[i])\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        attentiont = torch.zeros_like(attention[0])\n",
        "        for i in range (self.head):\n",
        "          attentiont+=attention[i]\n",
        "        attentiont/=self.head\n",
        "        # print(\"l\")\n",
        "        # print(self.W.shape)\n",
        "        # print(attentiont.shape)\n",
        "        # print(h.shape)\n",
        "        h_prime = torch.matmul(attentiont, h)\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        super(GAT, self).__init__()\n",
        "        self.hid = 8\n",
        "        self.conv1 = GATLayer(dataset.num_node_features, self.hid, alpha=0.2,head =8, dropout=0.6)\n",
        "        self.conv2 = GATLayer(self.hid, dataset.num_classes, concat=False, alpha=0.2,head =1, dropout=0.6)\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "mhC1-e_onvaj"
      },
      "id": "mhC1-e_onvaj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ae48ad-660d-4507-9aaf-c6f583f73060",
      "metadata": {
        "id": "08ae48ad-660d-4507-9aaf-c6f583f73060"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "\n",
        "# Load the CORA dataset\n",
        "dataset = Planetoid(root='./data/Planetoid', name='Cora', transform=T.NormalizeFeatures())\n",
        "data = dataset[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIywjvMqGhmC",
        "outputId": "9d43ed4c-9eb9-462a-dc2a-b96d32ce5092"
      },
      "id": "hIywjvMqGhmC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = data.y.cpu().numpy()\n",
        "label_to_indices = {}\n",
        "for i in range(dataset.num_classes):\n",
        "    label_to_indices[i] = (labels == i).nonzero()[0]\n",
        "import numpy as np\n",
        "train_indices = []\n",
        "val_indices = []\n",
        "test_indices = []\n",
        "np.random.seed(42)\n",
        "\n",
        "for label, indices in label_to_indices.items():\n",
        "    np.random.shuffle(indices)\n",
        "    train_indices.extend(indices[:20])\n",
        "    val_indices.extend(indices[20:20+500])\n",
        "    test_indices.extend(indices[20+500:20+500+1000])\n",
        "import torch\n",
        "\n",
        "train_mask = torch.zeros_like(data.y, dtype=torch.bool)\n",
        "train_mask[train_indices] = True\n",
        "\n",
        "val_mask = torch.zeros_like(data.y, dtype=torch.bool)\n",
        "val_mask[val_indices] = True\n",
        "\n",
        "test_mask = torch.zeros_like(data.y, dtype=torch.bool)\n",
        "test_mask[test_indices] = True\n",
        "from torch_geometric.data import Data\n",
        "data = Data(x=data.x, edge_index=data.edge_index, y=data.y, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
        "model = GAT(dataset)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.0005)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    # print(out.shape)\n",
        "    loss = criterion(out[train_mask], data.y[train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "@torch.no_grad()\n",
        "def validate():\n",
        "    model.eval()\n",
        "    raw = model(data)\n",
        "    accs = []\n",
        "    for mask in [train_mask, val_mask, test_mask]:\n",
        "        pred = raw[mask].max(1)[1]\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "for e in range(1, 201):\n",
        "    l = train()\n",
        "    t, v, te = validate()\n",
        "    print(f'Epoch: {e}, Loss: {l:.4f}, Train: {t:.4f}, Val: {v:.4f}, Test: {te:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J649fI-nnEIH",
        "outputId": "6c4c73c8-59bf-4866-adea-ada22138802a"
      },
      "id": "J649fI-nnEIH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 1.9477, Train: 0.2786, Val: 0.1780, Test: 0.2349\n",
            "Epoch: 002, Loss: 1.9443, Train: 0.4000, Val: 0.2881, Test: 0.3490\n",
            "Epoch: 003, Loss: 1.9363, Train: 0.6286, Val: 0.4150, Test: 0.4497\n",
            "Epoch: 004, Loss: 1.9351, Train: 0.7286, Val: 0.5207, Test: 0.5235\n",
            "Epoch: 005, Loss: 1.9265, Train: 0.7857, Val: 0.5714, Test: 0.5772\n",
            "Epoch: 006, Loss: 1.9215, Train: 0.8143, Val: 0.6167, Test: 0.5872\n",
            "Epoch: 007, Loss: 1.9154, Train: 0.8429, Val: 0.6502, Test: 0.6107\n",
            "Epoch: 008, Loss: 1.9134, Train: 0.8500, Val: 0.6696, Test: 0.6376\n",
            "Epoch: 009, Loss: 1.9086, Train: 0.8571, Val: 0.6863, Test: 0.6510\n",
            "Epoch: 010, Loss: 1.8979, Train: 0.8786, Val: 0.6960, Test: 0.6510\n",
            "Epoch: 011, Loss: 1.8955, Train: 0.9000, Val: 0.7048, Test: 0.6745\n",
            "Epoch: 012, Loss: 1.8936, Train: 0.9143, Val: 0.7093, Test: 0.6846\n",
            "Epoch: 013, Loss: 1.8935, Train: 0.9143, Val: 0.7141, Test: 0.7081\n",
            "Epoch: 014, Loss: 1.8640, Train: 0.9286, Val: 0.7203, Test: 0.7114\n",
            "Epoch: 015, Loss: 1.8718, Train: 0.9214, Val: 0.7251, Test: 0.7181\n",
            "Epoch: 016, Loss: 1.8691, Train: 0.9214, Val: 0.7264, Test: 0.7181\n",
            "Epoch: 017, Loss: 1.8672, Train: 0.9357, Val: 0.7291, Test: 0.7148\n",
            "Epoch: 018, Loss: 1.8497, Train: 0.9429, Val: 0.7308, Test: 0.7114\n",
            "Epoch: 019, Loss: 1.8446, Train: 0.9429, Val: 0.7313, Test: 0.7181\n",
            "Epoch: 020, Loss: 1.8314, Train: 0.9429, Val: 0.7326, Test: 0.7248\n",
            "Epoch: 021, Loss: 1.8477, Train: 0.9429, Val: 0.7366, Test: 0.7148\n",
            "Epoch: 022, Loss: 1.8217, Train: 0.9429, Val: 0.7361, Test: 0.7047\n",
            "Epoch: 023, Loss: 1.8074, Train: 0.9429, Val: 0.7352, Test: 0.6846\n",
            "Epoch: 024, Loss: 1.8445, Train: 0.9429, Val: 0.7344, Test: 0.6678\n",
            "Epoch: 025, Loss: 1.8035, Train: 0.9429, Val: 0.7366, Test: 0.6577\n",
            "Epoch: 026, Loss: 1.7931, Train: 0.9429, Val: 0.7361, Test: 0.6510\n",
            "Epoch: 027, Loss: 1.8119, Train: 0.9357, Val: 0.7348, Test: 0.6443\n",
            "Epoch: 028, Loss: 1.7914, Train: 0.9357, Val: 0.7344, Test: 0.6376\n",
            "Epoch: 029, Loss: 1.7998, Train: 0.9357, Val: 0.7335, Test: 0.6309\n",
            "Epoch: 030, Loss: 1.7657, Train: 0.9429, Val: 0.7344, Test: 0.6208\n",
            "Epoch: 031, Loss: 1.7339, Train: 0.9429, Val: 0.7330, Test: 0.6174\n",
            "Epoch: 032, Loss: 1.7632, Train: 0.9500, Val: 0.7291, Test: 0.6208\n",
            "Epoch: 033, Loss: 1.7452, Train: 0.9500, Val: 0.7273, Test: 0.6309\n",
            "Epoch: 034, Loss: 1.7233, Train: 0.9500, Val: 0.7260, Test: 0.6376\n",
            "Epoch: 035, Loss: 1.7266, Train: 0.9500, Val: 0.7256, Test: 0.6477\n",
            "Epoch: 036, Loss: 1.7225, Train: 0.9500, Val: 0.7260, Test: 0.6477\n",
            "Epoch: 037, Loss: 1.7192, Train: 0.9500, Val: 0.7238, Test: 0.6510\n",
            "Epoch: 038, Loss: 1.7579, Train: 0.9429, Val: 0.7225, Test: 0.6443\n",
            "Epoch: 039, Loss: 1.6912, Train: 0.9429, Val: 0.7238, Test: 0.6443\n",
            "Epoch: 040, Loss: 1.7244, Train: 0.9429, Val: 0.7256, Test: 0.6443\n",
            "Epoch: 041, Loss: 1.7254, Train: 0.9429, Val: 0.7260, Test: 0.6409\n",
            "Epoch: 042, Loss: 1.7208, Train: 0.9429, Val: 0.7273, Test: 0.6376\n",
            "Epoch: 043, Loss: 1.7053, Train: 0.9500, Val: 0.7286, Test: 0.6409\n",
            "Epoch: 044, Loss: 1.6817, Train: 0.9500, Val: 0.7291, Test: 0.6409\n",
            "Epoch: 045, Loss: 1.6604, Train: 0.9500, Val: 0.7282, Test: 0.6409\n",
            "Epoch: 046, Loss: 1.6540, Train: 0.9500, Val: 0.7291, Test: 0.6443\n",
            "Epoch: 047, Loss: 1.6565, Train: 0.9500, Val: 0.7295, Test: 0.6443\n",
            "Epoch: 048, Loss: 1.6230, Train: 0.9571, Val: 0.7282, Test: 0.6409\n",
            "Epoch: 049, Loss: 1.6472, Train: 0.9571, Val: 0.7295, Test: 0.6342\n",
            "Epoch: 050, Loss: 1.6555, Train: 0.9571, Val: 0.7313, Test: 0.6275\n",
            "Epoch: 051, Loss: 1.6679, Train: 0.9571, Val: 0.7304, Test: 0.6275\n",
            "Epoch: 052, Loss: 1.6389, Train: 0.9571, Val: 0.7317, Test: 0.6208\n",
            "Epoch: 053, Loss: 1.6114, Train: 0.9571, Val: 0.7304, Test: 0.6141\n",
            "Epoch: 054, Loss: 1.6079, Train: 0.9571, Val: 0.7308, Test: 0.6141\n",
            "Epoch: 055, Loss: 1.5685, Train: 0.9571, Val: 0.7300, Test: 0.6141\n",
            "Epoch: 056, Loss: 1.6011, Train: 0.9571, Val: 0.7308, Test: 0.6040\n",
            "Epoch: 057, Loss: 1.5994, Train: 0.9571, Val: 0.7317, Test: 0.6107\n",
            "Epoch: 058, Loss: 1.5504, Train: 0.9571, Val: 0.7304, Test: 0.6141\n",
            "Epoch: 059, Loss: 1.5797, Train: 0.9571, Val: 0.7326, Test: 0.6141\n",
            "Epoch: 060, Loss: 1.5922, Train: 0.9571, Val: 0.7330, Test: 0.6174\n",
            "Epoch: 061, Loss: 1.6183, Train: 0.9571, Val: 0.7339, Test: 0.6208\n",
            "Epoch: 062, Loss: 1.5383, Train: 0.9571, Val: 0.7352, Test: 0.6242\n",
            "Epoch: 063, Loss: 1.5395, Train: 0.9571, Val: 0.7344, Test: 0.6376\n",
            "Epoch: 064, Loss: 1.5653, Train: 0.9571, Val: 0.7348, Test: 0.6409\n",
            "Epoch: 065, Loss: 1.5462, Train: 0.9643, Val: 0.7366, Test: 0.6443\n",
            "Epoch: 066, Loss: 1.4757, Train: 0.9643, Val: 0.7383, Test: 0.6510\n",
            "Epoch: 067, Loss: 1.5013, Train: 0.9571, Val: 0.7410, Test: 0.6510\n",
            "Epoch: 068, Loss: 1.5240, Train: 0.9500, Val: 0.7405, Test: 0.6510\n",
            "Epoch: 069, Loss: 1.4811, Train: 0.9500, Val: 0.7410, Test: 0.6443\n",
            "Epoch: 070, Loss: 1.4659, Train: 0.9500, Val: 0.7410, Test: 0.6443\n",
            "Epoch: 071, Loss: 1.5089, Train: 0.9500, Val: 0.7419, Test: 0.6510\n",
            "Epoch: 072, Loss: 1.4565, Train: 0.9500, Val: 0.7405, Test: 0.6544\n",
            "Epoch: 073, Loss: 1.4901, Train: 0.9500, Val: 0.7401, Test: 0.6577\n",
            "Epoch: 074, Loss: 1.5187, Train: 0.9500, Val: 0.7396, Test: 0.6611\n",
            "Epoch: 075, Loss: 1.4532, Train: 0.9500, Val: 0.7396, Test: 0.6611\n",
            "Epoch: 076, Loss: 1.4561, Train: 0.9500, Val: 0.7392, Test: 0.6611\n",
            "Epoch: 077, Loss: 1.4145, Train: 0.9500, Val: 0.7405, Test: 0.6611\n",
            "Epoch: 078, Loss: 1.4868, Train: 0.9500, Val: 0.7423, Test: 0.6644\n",
            "Epoch: 079, Loss: 1.4142, Train: 0.9571, Val: 0.7427, Test: 0.6644\n",
            "Epoch: 080, Loss: 1.4217, Train: 0.9643, Val: 0.7449, Test: 0.6644\n",
            "Epoch: 081, Loss: 1.3821, Train: 0.9571, Val: 0.7467, Test: 0.6678\n",
            "Epoch: 082, Loss: 1.4598, Train: 0.9643, Val: 0.7511, Test: 0.6678\n",
            "Epoch: 083, Loss: 1.3934, Train: 0.9643, Val: 0.7529, Test: 0.6644\n",
            "Epoch: 084, Loss: 1.4383, Train: 0.9643, Val: 0.7533, Test: 0.6644\n",
            "Epoch: 085, Loss: 1.4348, Train: 0.9643, Val: 0.7555, Test: 0.6611\n",
            "Epoch: 086, Loss: 1.4172, Train: 0.9643, Val: 0.7555, Test: 0.6544\n",
            "Epoch: 087, Loss: 1.4132, Train: 0.9643, Val: 0.7555, Test: 0.6477\n",
            "Epoch: 088, Loss: 1.5052, Train: 0.9643, Val: 0.7559, Test: 0.6477\n",
            "Epoch: 089, Loss: 1.4179, Train: 0.9643, Val: 0.7555, Test: 0.6477\n",
            "Epoch: 090, Loss: 1.4354, Train: 0.9643, Val: 0.7555, Test: 0.6477\n",
            "Epoch: 091, Loss: 1.4397, Train: 0.9643, Val: 0.7555, Test: 0.6510\n",
            "Epoch: 092, Loss: 1.4282, Train: 0.9643, Val: 0.7555, Test: 0.6510\n",
            "Epoch: 093, Loss: 1.4346, Train: 0.9643, Val: 0.7555, Test: 0.6544\n",
            "Epoch: 094, Loss: 1.4172, Train: 0.9643, Val: 0.7542, Test: 0.6544\n",
            "Epoch: 095, Loss: 1.3533, Train: 0.9714, Val: 0.7555, Test: 0.6544\n",
            "Epoch: 096, Loss: 1.4079, Train: 0.9714, Val: 0.7551, Test: 0.6544\n",
            "Epoch: 097, Loss: 1.3940, Train: 0.9714, Val: 0.7542, Test: 0.6544\n",
            "Epoch: 098, Loss: 1.3741, Train: 0.9714, Val: 0.7524, Test: 0.6443\n",
            "Epoch: 099, Loss: 1.2819, Train: 0.9714, Val: 0.7533, Test: 0.6409\n",
            "Epoch: 100, Loss: 1.3862, Train: 0.9714, Val: 0.7524, Test: 0.6309\n",
            "Epoch: 101, Loss: 1.3854, Train: 0.9714, Val: 0.7507, Test: 0.6309\n",
            "Epoch: 102, Loss: 1.3808, Train: 0.9714, Val: 0.7498, Test: 0.6309\n",
            "Epoch: 103, Loss: 1.3784, Train: 0.9714, Val: 0.7515, Test: 0.6208\n",
            "Epoch: 104, Loss: 1.3486, Train: 0.9714, Val: 0.7537, Test: 0.6074\n",
            "Epoch: 105, Loss: 1.3654, Train: 0.9714, Val: 0.7533, Test: 0.6040\n",
            "Epoch: 106, Loss: 1.3818, Train: 0.9714, Val: 0.7529, Test: 0.6007\n",
            "Epoch: 107, Loss: 1.2963, Train: 0.9714, Val: 0.7529, Test: 0.5973\n",
            "Epoch: 108, Loss: 1.3450, Train: 0.9714, Val: 0.7524, Test: 0.5973\n",
            "Epoch: 109, Loss: 1.3983, Train: 0.9714, Val: 0.7520, Test: 0.5973\n",
            "Epoch: 110, Loss: 1.3405, Train: 0.9714, Val: 0.7520, Test: 0.5973\n",
            "Epoch: 111, Loss: 1.3120, Train: 0.9714, Val: 0.7529, Test: 0.6007\n",
            "Epoch: 112, Loss: 1.3449, Train: 0.9714, Val: 0.7542, Test: 0.6007\n",
            "Epoch: 113, Loss: 1.2398, Train: 0.9714, Val: 0.7546, Test: 0.6007\n",
            "Epoch: 114, Loss: 1.2999, Train: 0.9714, Val: 0.7546, Test: 0.5973\n",
            "Epoch: 115, Loss: 1.3386, Train: 0.9714, Val: 0.7564, Test: 0.6007\n",
            "Epoch: 116, Loss: 1.3823, Train: 0.9714, Val: 0.7573, Test: 0.6007\n",
            "Epoch: 117, Loss: 1.2999, Train: 0.9714, Val: 0.7581, Test: 0.6007\n",
            "Epoch: 118, Loss: 1.3368, Train: 0.9714, Val: 0.7577, Test: 0.6107\n",
            "Epoch: 119, Loss: 1.2513, Train: 0.9714, Val: 0.7590, Test: 0.6208\n",
            "Epoch: 120, Loss: 1.2964, Train: 0.9714, Val: 0.7590, Test: 0.6242\n",
            "Epoch: 121, Loss: 1.3310, Train: 0.9714, Val: 0.7599, Test: 0.6275\n",
            "Epoch: 122, Loss: 1.2123, Train: 0.9714, Val: 0.7604, Test: 0.6342\n",
            "Epoch: 123, Loss: 1.2542, Train: 0.9714, Val: 0.7599, Test: 0.6376\n",
            "Epoch: 124, Loss: 1.3687, Train: 0.9714, Val: 0.7612, Test: 0.6443\n",
            "Epoch: 125, Loss: 1.2809, Train: 0.9714, Val: 0.7626, Test: 0.6443\n",
            "Epoch: 126, Loss: 1.2556, Train: 0.9714, Val: 0.7630, Test: 0.6443\n",
            "Epoch: 127, Loss: 1.3143, Train: 0.9714, Val: 0.7656, Test: 0.6477\n",
            "Epoch: 128, Loss: 1.2607, Train: 0.9714, Val: 0.7674, Test: 0.6611\n",
            "Epoch: 129, Loss: 1.3074, Train: 0.9714, Val: 0.7678, Test: 0.6678\n",
            "Epoch: 130, Loss: 1.2659, Train: 0.9714, Val: 0.7683, Test: 0.6711\n",
            "Epoch: 131, Loss: 1.2926, Train: 0.9714, Val: 0.7678, Test: 0.6745\n",
            "Epoch: 132, Loss: 1.3327, Train: 0.9714, Val: 0.7683, Test: 0.6745\n",
            "Epoch: 133, Loss: 1.3089, Train: 0.9714, Val: 0.7692, Test: 0.6745\n",
            "Epoch: 134, Loss: 1.2974, Train: 0.9714, Val: 0.7700, Test: 0.6711\n",
            "Epoch: 135, Loss: 1.2994, Train: 0.9714, Val: 0.7705, Test: 0.6745\n",
            "Epoch: 136, Loss: 1.2754, Train: 0.9786, Val: 0.7714, Test: 0.6711\n",
            "Epoch: 137, Loss: 1.2671, Train: 0.9786, Val: 0.7727, Test: 0.6711\n",
            "Epoch: 138, Loss: 1.2663, Train: 0.9786, Val: 0.7753, Test: 0.6745\n",
            "Epoch: 139, Loss: 1.2467, Train: 0.9786, Val: 0.7767, Test: 0.6745\n",
            "Epoch: 140, Loss: 1.2729, Train: 0.9786, Val: 0.7775, Test: 0.6745\n",
            "Epoch: 141, Loss: 1.2680, Train: 0.9786, Val: 0.7784, Test: 0.6745\n",
            "Epoch: 142, Loss: 1.2134, Train: 0.9786, Val: 0.7780, Test: 0.6745\n",
            "Epoch: 143, Loss: 1.2395, Train: 0.9786, Val: 0.7793, Test: 0.6745\n",
            "Epoch: 144, Loss: 1.2231, Train: 0.9786, Val: 0.7797, Test: 0.6745\n",
            "Epoch: 145, Loss: 1.2354, Train: 0.9786, Val: 0.7793, Test: 0.6745\n",
            "Epoch: 146, Loss: 1.3219, Train: 0.9786, Val: 0.7802, Test: 0.6745\n",
            "Epoch: 147, Loss: 1.2670, Train: 0.9786, Val: 0.7797, Test: 0.6745\n",
            "Epoch: 148, Loss: 1.2170, Train: 0.9786, Val: 0.7797, Test: 0.6812\n",
            "Epoch: 149, Loss: 1.1412, Train: 0.9786, Val: 0.7797, Test: 0.6812\n",
            "Epoch: 150, Loss: 1.2357, Train: 0.9786, Val: 0.7793, Test: 0.6846\n",
            "Epoch: 151, Loss: 1.1908, Train: 0.9786, Val: 0.7797, Test: 0.6846\n",
            "Epoch: 152, Loss: 1.2170, Train: 0.9786, Val: 0.7797, Test: 0.6846\n",
            "Epoch: 153, Loss: 1.2658, Train: 0.9786, Val: 0.7802, Test: 0.6879\n",
            "Epoch: 154, Loss: 1.2241, Train: 0.9786, Val: 0.7797, Test: 0.6879\n",
            "Epoch: 155, Loss: 1.1995, Train: 0.9786, Val: 0.7802, Test: 0.6879\n",
            "Epoch: 156, Loss: 1.2537, Train: 0.9786, Val: 0.7802, Test: 0.6846\n",
            "Epoch: 157, Loss: 1.2152, Train: 0.9786, Val: 0.7797, Test: 0.6812\n",
            "Epoch: 158, Loss: 1.2211, Train: 0.9786, Val: 0.7789, Test: 0.6779\n",
            "Epoch: 159, Loss: 1.2085, Train: 0.9786, Val: 0.7793, Test: 0.6779\n",
            "Epoch: 160, Loss: 1.2400, Train: 0.9786, Val: 0.7789, Test: 0.6745\n",
            "Epoch: 161, Loss: 1.2113, Train: 0.9786, Val: 0.7793, Test: 0.6779\n",
            "Epoch: 162, Loss: 1.1105, Train: 0.9786, Val: 0.7802, Test: 0.6812\n",
            "Epoch: 163, Loss: 1.1601, Train: 0.9786, Val: 0.7797, Test: 0.6779\n",
            "Epoch: 164, Loss: 1.2480, Train: 0.9786, Val: 0.7767, Test: 0.6745\n",
            "Epoch: 165, Loss: 1.2055, Train: 0.9786, Val: 0.7767, Test: 0.6779\n",
            "Epoch: 166, Loss: 1.2181, Train: 0.9786, Val: 0.7784, Test: 0.6745\n",
            "Epoch: 167, Loss: 1.2036, Train: 0.9786, Val: 0.7789, Test: 0.6745\n",
            "Epoch: 168, Loss: 1.2485, Train: 0.9786, Val: 0.7784, Test: 0.6711\n",
            "Epoch: 169, Loss: 1.1655, Train: 0.9786, Val: 0.7784, Test: 0.6678\n",
            "Epoch: 170, Loss: 1.2090, Train: 0.9786, Val: 0.7797, Test: 0.6678\n",
            "Epoch: 171, Loss: 1.2650, Train: 0.9786, Val: 0.7780, Test: 0.6644\n",
            "Epoch: 172, Loss: 1.1752, Train: 0.9786, Val: 0.7784, Test: 0.6644\n",
            "Epoch: 173, Loss: 1.1998, Train: 0.9786, Val: 0.7806, Test: 0.6577\n",
            "Epoch: 174, Loss: 1.1732, Train: 0.9786, Val: 0.7815, Test: 0.6577\n",
            "Epoch: 175, Loss: 1.2330, Train: 0.9786, Val: 0.7811, Test: 0.6544\n",
            "Epoch: 176, Loss: 1.3063, Train: 0.9786, Val: 0.7819, Test: 0.6611\n",
            "Epoch: 177, Loss: 1.1571, Train: 0.9786, Val: 0.7824, Test: 0.6611\n",
            "Epoch: 178, Loss: 1.2193, Train: 0.9786, Val: 0.7819, Test: 0.6745\n",
            "Epoch: 179, Loss: 1.1519, Train: 0.9786, Val: 0.7824, Test: 0.6745\n",
            "Epoch: 180, Loss: 1.2639, Train: 0.9786, Val: 0.7819, Test: 0.6779\n",
            "Epoch: 181, Loss: 1.2797, Train: 0.9786, Val: 0.7824, Test: 0.6779\n",
            "Epoch: 182, Loss: 1.1595, Train: 0.9786, Val: 0.7819, Test: 0.6779\n",
            "Epoch: 183, Loss: 1.1814, Train: 0.9786, Val: 0.7828, Test: 0.6812\n",
            "Epoch: 184, Loss: 1.2400, Train: 0.9786, Val: 0.7837, Test: 0.6846\n",
            "Epoch: 185, Loss: 1.1693, Train: 0.9786, Val: 0.7833, Test: 0.6846\n",
            "Epoch: 186, Loss: 1.2304, Train: 0.9786, Val: 0.7828, Test: 0.6846\n",
            "Epoch: 187, Loss: 1.3258, Train: 0.9786, Val: 0.7850, Test: 0.6879\n",
            "Epoch: 188, Loss: 1.2312, Train: 0.9786, Val: 0.7850, Test: 0.6980\n",
            "Epoch: 189, Loss: 1.2015, Train: 0.9786, Val: 0.7863, Test: 0.6980\n",
            "Epoch: 190, Loss: 1.1427, Train: 0.9786, Val: 0.7872, Test: 0.7013\n",
            "Epoch: 191, Loss: 1.1943, Train: 0.9786, Val: 0.7890, Test: 0.7047\n",
            "Epoch: 192, Loss: 1.2296, Train: 0.9786, Val: 0.7890, Test: 0.7047\n",
            "Epoch: 193, Loss: 1.2506, Train: 0.9786, Val: 0.7907, Test: 0.7047\n",
            "Epoch: 194, Loss: 1.1441, Train: 0.9786, Val: 0.7921, Test: 0.7081\n",
            "Epoch: 195, Loss: 1.2044, Train: 0.9786, Val: 0.7934, Test: 0.7081\n",
            "Epoch: 196, Loss: 1.1867, Train: 0.9786, Val: 0.7930, Test: 0.7081\n",
            "Epoch: 197, Loss: 1.1823, Train: 0.9786, Val: 0.7930, Test: 0.7081\n",
            "Epoch: 198, Loss: 1.1474, Train: 0.9786, Val: 0.7925, Test: 0.7081\n",
            "Epoch: 199, Loss: 1.2361, Train: 0.9786, Val: 0.7916, Test: 0.7081\n",
            "Epoch: 200, Loss: 1.2168, Train: 0.9786, Val: 0.7921, Test: 0.7148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "|print(data.x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0YV2zqJIptz",
        "outputId": "9870cbde-f93c-4c1b-e53b-9000ea5c5403"
      },
      "id": "k0YV2zqJIptz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2708, 1433])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch-geometric"
      ],
      "metadata": {
        "id": "1EHlwAEvnS7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d335accb-73f5-442e-bb35-c4f5397347e1"
      },
      "id": "1EHlwAEvnS7D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "khZcdZkR_Emy"
      },
      "id": "khZcdZkR_Emy",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}